#!/usr/bin/env python
# coding=utf-8
"""
Web3 çˆ¬è™«æµ‹è¯•è„šæœ¬

æµ‹è¯• ChainCatcher å’Œ ME News çˆ¬è™«æ˜¯å¦æ­£å¸¸å·¥ä½œ
è¿è¡Œæ–¹å¼: python test_web3_crawler.py
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from datetime import datetime


def print_separator(title: str = ""):
    """æ‰“å°åˆ†éš”çº¿"""
    if title:
        print(f"\n{'='*60}")
        print(f"  {title}")
        print(f"{'='*60}")
    else:
        print("-" * 60)


def print_item(item, index: int):
    """æ‰“å°å•ä¸ªæ¡ç›®"""
    print(f"\n[{index}] {item.title[:80]}{'...' if len(item.title) > 80 else ''}")
    print(f"    URL: {item.url[:60]}{'...' if len(item.url) > 60 else ''}")
    if item.published_at:
        print(f"    æ—¶é—´: {item.published_at}")
    if item.author:
        print(f"    ä½œè€…: {item.author}")
    if item.summary:
        summary = item.summary[:100] + "..." if len(item.summary) > 100 else item.summary
        print(f"    æ‘˜è¦: {summary}")


def test_chaincatcher():
    """æµ‹è¯• ChainCatcher çˆ¬è™«"""
    print_separator("æµ‹è¯• ChainCatcher çˆ¬è™«")

    try:
        from trendradar.crawler.web3.chaincatcher import ChainCatcherCrawler

        crawler = ChainCatcherCrawler(timeout=30)
        print(f"çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
        print(f"æ•°æ®æº ID: {crawler.source_id}")
        print(f"æ•°æ®æºåç§°: {crawler.source_name}")

        print("\næ­£åœ¨æŠ“å–æ•°æ®...")
        items = crawler.crawl(max_items=10)

        print(f"\næŠ“å–ç»“æœ: å…± {len(items)} æ¡")

        if items:
            print("\nå‰ 5 æ¡å†…å®¹:")
            for i, item in enumerate(items[:5], 1):
                print_item(item, i)
            return True
        else:
            print("âš ï¸ æœªæŠ“å–åˆ°æ•°æ®")
            return False

    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…ä¾èµ–: pip install beautifulsoup4 requests")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_menews():
    """æµ‹è¯• ME News çˆ¬è™«"""
    print_separator("æµ‹è¯• ME News çˆ¬è™«")

    try:
        from trendradar.crawler.web3.menews import MeNewsCrawler

        crawler = MeNewsCrawler(timeout=30)
        print(f"çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
        print(f"æ•°æ®æº ID: {crawler.source_id}")
        print(f"æ•°æ®æºåç§°: {crawler.source_name}")

        print("\næ­£åœ¨æŠ“å–æ•°æ®...")
        items = crawler.crawl(max_items=10)

        print(f"\næŠ“å–ç»“æœ: å…± {len(items)} æ¡")

        if items:
            print("\nå‰ 5 æ¡å†…å®¹:")
            for i, item in enumerate(items[:5], 1):
                print_item(item, i)
            return True
        else:
            print("âš ï¸ æœªæŠ“å–åˆ°æ•°æ®ï¼ˆå¯èƒ½æ˜¯ç½‘ç«™éœ€è¦ JavaScript æ¸²æŸ“ï¼‰")
            return False

    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…ä¾èµ–: pip install beautifulsoup4 requests")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_web3_fetcher():
    """æµ‹è¯• Web3Fetcher æ•´åˆæŠ“å–"""
    print_separator("æµ‹è¯• Web3Fetcher æ•´åˆæŠ“å–")

    try:
        from trendradar.crawler.web3.fetcher import Web3Fetcher, Web3FeedConfig

        # é…ç½®ä¿¡æ¯æº
        feeds = [
            Web3FeedConfig(
                id="chaincatcher",
                name="ChainCatcher é“¾æ•æ‰‹",
                url="https://www.chaincatcher.com/news",
                crawler_type="chaincatcher",
                max_items=10,
                enabled=True,
            ),
            Web3FeedConfig(
                id="menews",
                name="ME News",
                url="https://www.me.news/news",
                crawler_type="menews",
                max_items=10,
                enabled=True,
            ),
        ]

        fetcher = Web3Fetcher(
            feeds=feeds,
            request_interval=2000,
            timeout=30,
        )

        print(f"Fetcher åˆå§‹åŒ–æˆåŠŸï¼Œé…ç½®äº† {len(feeds)} ä¸ªä¿¡æ¯æº")

        print("\næ­£åœ¨æŠ“å–æ‰€æœ‰ä¿¡æ¯æº...")
        rss_data = fetcher.fetch_all()

        print(f"\næŠ“å–æ—¥æœŸ: {rss_data.date}")
        print(f"æŠ“å–æ—¶é—´: {rss_data.crawl_time}")
        print(f"æˆåŠŸæ•°é‡: {len(rss_data.items)} ä¸ªæº")
        print(f"å¤±è´¥æ•°é‡: {len(rss_data.failed_ids)} ä¸ªæº")
        print(f"æ€»æ¡ç›®æ•°: {rss_data.get_total_count()} æ¡")

        if rss_data.failed_ids:
            print(f"\nå¤±è´¥çš„æº: {', '.join(rss_data.failed_ids)}")

        # æ˜¾ç¤ºæ¯ä¸ªæºçš„ç»“æœ
        for feed_id, items in rss_data.items.items():
            feed_name = rss_data.id_to_name.get(feed_id, feed_id)
            print(f"\n--- {feed_name} ({len(items)} æ¡) ---")
            for i, item in enumerate(items[:3], 1):
                print(f"  [{i}] {item.title[:60]}{'...' if len(item.title) > 60 else ''}")

        return rss_data.get_total_count() > 0

    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_config_loading():
    """æµ‹è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½"""
    print_separator("æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½")

    try:
        import yaml
        config_path = os.path.join(
            os.path.dirname(__file__),
            "config",
            "web3_sources.yaml"
        )

        if not os.path.exists(config_path):
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False

        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        web3_config = config.get("web3", {})
        print(f"é…ç½®åŠ è½½æˆåŠŸ")
        print(f"å¯ç”¨çŠ¶æ€: {web3_config.get('enabled', False)}")
        print(f"è¯·æ±‚é—´éš”: {web3_config.get('request_interval', 0)} ms")

        feeds = web3_config.get("feeds", [])
        print(f"é…ç½®çš„ä¿¡æ¯æºæ•°é‡: {len(feeds)}")

        for feed in feeds:
            status = "âœ“" if feed.get("enabled", True) else "âœ—"
            print(f"  {status} {feed.get('name', 'Unknown')} ({feed.get('crawler_type', 'unknown')})")

        return True

    except ImportError:
        print("âš ï¸ éœ€è¦å®‰è£… pyyaml: pip install pyyaml")
        return False
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print(f"\n{'#'*60}")
    print(f"#  Web3 çˆ¬è™«æµ‹è¯•")
    print(f"#  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'#'*60}")

    results = {}

    # æµ‹è¯•é…ç½®åŠ è½½
    results["é…ç½®åŠ è½½"] = test_config_loading()

    # æµ‹è¯• ChainCatcher
    results["ChainCatcher"] = test_chaincatcher()

    # æµ‹è¯• ME News
    results["ME News"] = test_menews()

    # æµ‹è¯•æ•´åˆæŠ“å–
    results["Web3Fetcher"] = test_web3_fetcher()

    # æ‰“å°æµ‹è¯•ç»“æœæ±‡æ€»
    print_separator("æµ‹è¯•ç»“æœæ±‡æ€»")

    all_passed = True
    for name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
        if not passed:
            all_passed = False

    print()
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())
